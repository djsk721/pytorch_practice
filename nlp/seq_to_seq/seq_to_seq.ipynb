{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Transformer 와 TORCHTEXT 로 시퀀스-투-시퀀스(SEQUENCE-TO-SEQUENCE) 모델링하기\n",
    "\n",
    "-   트랜스포머 모델은 다양한 시퀀스-투-시퀀스 문제들에서 더 별렬화(parallelizable)가 가능\n",
    "-   순환 신경망(RNN; Recurrent Neural Network)과 비교하여 더 나은 성능을 보임\n",
    "-   nn.Transformer 모듈은 입력(input)과 출력(output) 사이의 전역적인 의존성(global dependencies)을 나타내기 위하여(nn.MultiheadAttention으로 구현된) 어텐션(attention) 메커니즘에 의존\n",
    "![](https://velog.velcdn.com/images/afsd721/post/78e85ae8-68ff-43ed-af9f-25abc28bbd05/image.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 정의하기\n",
    "-   언어 모델링 과제는 주어진 단어 다음에 이어지는 단어 시퀀스를 따를 가능성(likelihood)에 대한 확률을 할당하는 것.\n",
    "-   토큰(token)들의 시퀀스가 임베딩(embedding) 층으로 전달되며, 이어서 포지셔널 인코딩(positional encoding) 레이어가 각 단어의 순서를 설명\n",
    "-   nn.TransformerEncoder는 여러 개의 nn.TransformerEncoderLayer 로 구성되어 있다.\n",
    "-   nn.TransformerEncoder 내부의 셀프-어텐션(self-attention) 층들은 시퀀스 안에서의 이전 포지션에만 집중하도록 허용\n",
    "-   따라서, 입력 순서와 함께 정사각 형태의 어텐션 마스크(attention mask)가 필요하다.\n",
    "-   이후의 포지션에 있는 모든 토큰들은 마스킹 되어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimju\\OneDrive\\바탕 화면\\Study\\파이썬\\machine learning\\pytorch_practice\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        # 시퀀스 안에서 토큰의 상대적인 또는 절대적인 포지션에 대한 어떤 정보를 주입\n",
    "        # nn.Embedding 과 동일한 차원을 가짐\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드하고 배치 만들기\n",
    "\n",
    "-   어휘(vocab) 객체는 훈련 데이터셋(train dataset)에 의하여 만들어지고, 토큰(token)을 텐서(tensor)로 수치화하는데 사용된다.\n",
    "-   Wikitext-2 에서 보기 드믄 토큰(rare token)은 <unk> (Unknown)으로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터를 bach_size 단위로 나누고, 나머지는 버림\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 입력(input)과 타겟(target) 시퀀스를 생성하기 위한 함수들\n",
    "-   get_batch() 함수는 트랜스포머 모델을 위한 입력-타겟 시퀀스 쌍(Pair)을 생성한다.\n",
    "-   이 함수는 소스 데이터를 시간을 통한 역전파(BPTT; BackPropagation Though Time) 길이를 가진 덩어리로 세분화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인스턴스(instance) 초기화하기\n",
    "-   모델의 하이퍼파라미터(hyperparameter)는 아래와 같이 정의\n",
    "-   단어 사이즈는 단어 오브젝트의 길이와 일치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # 단어 사전(어휘집)의 크기\n",
    "emsize = 200 # 임베딩 차원\n",
    "d_hid = 200 # nn.TransformerEncoder 에서 피드포워드 네트워크(feedforward network) 모델의 차원\n",
    "nlayers = 2 # nn.TransformerEncoder 내부의 nn.TransformerEncoderLayer 개수\n",
    "nhead = 2 # nn.MultiheadAttention의 헤드 개수\n",
    "dropout = 0.2 # 드랍아웃(dropout) 확률\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 실행하기\n",
    "## 예시에서의 하이퍼파라미터\n",
    "-   CrossEntropyLoss를 SGD 옵티마이저와 함께 사용\n",
    "-   학습률은 5.0로 초기화 하였으며 StepLR 스케줄러를 사용\n",
    "-   학습하는동안 nn.utils.clip_grad_norm_을 사용하여 기울기(gradient)가 폭발(exploding)하지 않도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.5  # 학습률(learning rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # 학습 모드 시작\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt:  # 마지막 배치에만 적용\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # 평가 모드 시작\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 0.50 | ms/batch  6.64 | loss  5.43 | ppl   227.49\n",
      "| epoch   1 |   400/ 2928 batches | lr 0.50 | ms/batch  5.23 | loss  5.42 | ppl   225.38\n",
      "| epoch   1 |   600/ 2928 batches | lr 0.50 | ms/batch  5.26 | loss  5.21 | ppl   183.85\n",
      "| epoch   1 |   800/ 2928 batches | lr 0.50 | ms/batch  5.21 | loss  5.27 | ppl   194.44\n",
      "| epoch   1 |  1000/ 2928 batches | lr 0.50 | ms/batch  5.13 | loss  5.20 | ppl   182.07\n",
      "| epoch   1 |  1200/ 2928 batches | lr 0.50 | ms/batch  5.08 | loss  5.23 | ppl   187.61\n",
      "| epoch   1 |  1400/ 2928 batches | lr 0.50 | ms/batch  5.07 | loss  5.24 | ppl   189.09\n",
      "| epoch   1 |  1600/ 2928 batches | lr 0.50 | ms/batch  5.07 | loss  5.26 | ppl   192.56\n",
      "| epoch   1 |  1800/ 2928 batches | lr 0.50 | ms/batch  5.06 | loss  5.21 | ppl   183.16\n",
      "| epoch   1 |  2000/ 2928 batches | lr 0.50 | ms/batch  5.04 | loss  5.21 | ppl   183.02\n",
      "| epoch   1 |  2200/ 2928 batches | lr 0.50 | ms/batch  5.06 | loss  5.06 | ppl   157.67\n",
      "| epoch   1 |  2400/ 2928 batches | lr 0.50 | ms/batch  5.05 | loss  5.17 | ppl   175.34\n",
      "| epoch   1 |  2600/ 2928 batches | lr 0.50 | ms/batch  5.03 | loss  5.15 | ppl   172.94\n",
      "| epoch   1 |  2800/ 2928 batches | lr 0.50 | ms/batch  5.08 | loss  5.07 | ppl   159.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 15.94s | valid loss  5.41 | valid ppl   223.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 0.47 | ms/batch  5.07 | loss  5.29 | ppl   198.00\n",
      "| epoch   2 |   400/ 2928 batches | lr 0.47 | ms/batch  5.05 | loss  5.30 | ppl   200.29\n",
      "| epoch   2 |   600/ 2928 batches | lr 0.47 | ms/batch  5.05 | loss  5.10 | ppl   164.10\n",
      "| epoch   2 |   800/ 2928 batches | lr 0.47 | ms/batch  5.02 | loss  5.17 | ppl   176.12\n",
      "| epoch   2 |  1000/ 2928 batches | lr 0.47 | ms/batch  5.02 | loss  5.11 | ppl   166.24\n",
      "| epoch   2 |  1200/ 2928 batches | lr 0.47 | ms/batch  5.04 | loss  5.15 | ppl   173.24\n",
      "| epoch   2 |  1400/ 2928 batches | lr 0.47 | ms/batch  5.06 | loss  5.17 | ppl   175.27\n",
      "| epoch   2 |  1600/ 2928 batches | lr 0.47 | ms/batch  5.08 | loss  5.19 | ppl   179.02\n",
      "| epoch   2 |  1800/ 2928 batches | lr 0.47 | ms/batch  5.13 | loss  5.14 | ppl   170.79\n",
      "| epoch   2 |  2000/ 2928 batches | lr 0.47 | ms/batch  5.13 | loss  5.15 | ppl   171.97\n",
      "| epoch   2 |  2200/ 2928 batches | lr 0.47 | ms/batch  5.14 | loss  5.00 | ppl   148.10\n",
      "| epoch   2 |  2400/ 2928 batches | lr 0.47 | ms/batch  5.12 | loss  5.11 | ppl   165.69\n",
      "| epoch   2 |  2600/ 2928 batches | lr 0.47 | ms/batch  5.13 | loss  5.11 | ppl   165.21\n",
      "| epoch   2 |  2800/ 2928 batches | lr 0.47 | ms/batch  5.13 | loss  5.04 | ppl   153.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 15.59s | valid loss  5.38 | valid ppl   217.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 0.45 | ms/batch  5.10 | loss  5.23 | ppl   186.99\n",
      "| epoch   3 |   400/ 2928 batches | lr 0.45 | ms/batch  5.08 | loss  5.24 | ppl   189.13\n",
      "| epoch   3 |   600/ 2928 batches | lr 0.45 | ms/batch  5.04 | loss  5.05 | ppl   155.31\n",
      "| epoch   3 |   800/ 2928 batches | lr 0.45 | ms/batch  5.08 | loss  5.12 | ppl   166.69\n",
      "| epoch   3 |  1000/ 2928 batches | lr 0.45 | ms/batch  5.51 | loss  5.06 | ppl   157.59\n",
      "| epoch   3 |  1200/ 2928 batches | lr 0.45 | ms/batch  5.69 | loss  5.10 | ppl   164.42\n",
      "| epoch   3 |  1400/ 2928 batches | lr 0.45 | ms/batch  5.38 | loss  5.12 | ppl   166.82\n",
      "| epoch   3 |  1600/ 2928 batches | lr 0.45 | ms/batch  5.30 | loss  5.14 | ppl   171.40\n",
      "| epoch   3 |  1800/ 2928 batches | lr 0.45 | ms/batch  5.47 | loss  5.10 | ppl   164.07\n",
      "| epoch   3 |  2000/ 2928 batches | lr 0.45 | ms/batch  5.28 | loss  5.10 | ppl   164.23\n",
      "| epoch   3 |  2200/ 2928 batches | lr 0.45 | ms/batch  5.29 | loss  4.95 | ppl   141.84\n",
      "| epoch   3 |  2400/ 2928 batches | lr 0.45 | ms/batch  5.01 | loss  5.07 | ppl   158.95\n",
      "| epoch   3 |  2600/ 2928 batches | lr 0.45 | ms/batch  5.01 | loss  5.07 | ppl   158.66\n",
      "| epoch   3 |  2800/ 2928 batches | lr 0.45 | ms/batch  4.95 | loss  5.00 | ppl   148.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 15.96s | valid loss  5.36 | valid ppl   213.71\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가 데이터셋으로 모델을 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.28 | test ppl   195.81\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bec884793e722188830e2368a860f43c511541d0c2a75aea5e9314e17690de0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
